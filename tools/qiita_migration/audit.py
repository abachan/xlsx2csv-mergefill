# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª
!pip install tqdm

import requests
from bs4 import BeautifulSoup
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# å…¥åŠ›
target_exam = input("æ¤œç´¢ã™ã‚‹è©¦é¨“åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")
category_name = input("ã‚«ãƒ†ã‚´ãƒªåã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")
max_page = int(input("æœ€çµ‚ãƒšãƒ¼ã‚¸ç•ªå·ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: "))
start_url = f'https://www.examtopics.com/discussions/{category_name}/'

# å‡¦ç†å¯¾è±¡ã®ãƒšãƒ¼ã‚¸URLã‚’åé›†
all_urls = [f"{start_url}{i}" for i in range(1, max_page + 1)]

# å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«
output_filename = f'ExamTopics_{target_exam}_URL.txt'
output_filepath = os.path.join('/content', output_filename)
with open(output_filepath, mode='w') as f:
    pass  # ãƒ•ã‚¡ã‚¤ãƒ«åˆæœŸåŒ–

# å„ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒªãƒ³ã‚¯æŠ½å‡ºï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
def fetch_and_extract_links(url, target_exam):
    try:
        # User-Agentãƒ˜ãƒƒãƒ€ãƒ¼ã®è¨­å®š
        # "https://whatmyuseragent.com/"ãªã©ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€è‡ªèº«ã®User-Agentã‚’è¨­å®šã—ã¦ãã ã•ã„
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36 Edg/140.0.0.0'
        }
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'lxml')
        elements = soup.select('a.discussion-link')
        matches = []
        for element in elements:
            link_text = element.get_text()
            link_href = element.get('href')
            if target_exam.lower() in link_text.lower() and link_href:
                if not link_href.startswith('http'):
                    link_href = requests.compat.urljoin(url, link_href)
                matches.append(link_href)
        return matches
    except Exception as e:
        print(f"Error processing {url}: {e}")
        return []

# ä¸¦åˆ—å‡¦ç†ï¼‹é€²æ—è¡¨ç¤ºï¼‹ãƒãƒƒãƒæ›¸ãè¾¼ã¿
all_matches = []
with ThreadPoolExecutor(max_workers=10) as executor:
    futures = [executor.submit(fetch_and_extract_links, url, target_exam) for url in all_urls]
    for future in tqdm(as_completed(futures), total=len(futures), desc="ğŸ” ãƒšãƒ¼ã‚¸å‡¦ç†ä¸­", unit="ãƒšãƒ¼ã‚¸"):
        all_matches.extend(future.result())

# ä¸€æ‹¬æ›¸ãè¾¼ã¿
with open(output_filepath, mode='a') as f:
    for link in all_matches:
        print(link, file=f)

# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
from google.colab import files
if os.path.exists(output_filepath):
    files.download(output_filepath)
else:
    print(f"Output file not found: {output_filepath}")

print("ğŸ‰ å®Œäº†ã—ã¾ã—ãŸï¼")